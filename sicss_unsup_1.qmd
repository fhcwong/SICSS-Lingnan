---
title: "sicss_unsup_1"
date: June 18, 2025
instructor: Frankie Ho Chun WONG
---

## Text as data

With quanteda (ref: <https://quanteda.io/articles/pkgdown/examples/plotting.html>, <https://quanteda.io/articles/pkgdown/examples/plotting.html>)

This demo explores text data with word frequency (count data of words).

**Load packages**

```{r}
Sys.setLanguage("en")

library("quanteda")
library("quanteda.textplots")
library("quanteda.textstats")
library("ggplot2")
```

#Begin

**Obtain sample data**

```{r}
data<-data_corpus_inaugural
attributes(data)$docvar |> head() #same as head(attributes(data)$docvar)
```

**Tokenize text and create a document-feature matrix**

```{r}
dfmat_inaug <- corpus_subset(data, (Year >= 1946 & Year <= 1990)) |>
    tokens(remove_punct = TRUE) |>
    tokens_remove(pattern = stopwords('english')) |>
    dfm() |>
    dfm_trim(min_termfreq = 10, verbose = FALSE)
head(dfmat_inaug)
```

**Text frequency**

```{r}
text_freq <- textstat_frequency(dfmat_inaug, n = 20)
text_freq
ggplot(text_freq, aes(x = frequency, y = reorder(feature, frequency))) +
    geom_point() + 
    labs(x = "Frequency", y = "Feature")
```

### **Word cloud**

```{r}
set.seed(100)
textplot_wordcloud(dfmat_inaug)
```

**Word Cloud: Compare the presidents**

```{r}
#who are the presidents?
#docvars(data)$President |> unique()
#find the exact speech
#attributes(data)$docvar$docid_ |> as.character() |> writeLines()
```

Compare between presidents

```{r}
corpus_subset(data, 
              President %in% c("Trump", "Obama", "Biden")) |>
    tokens(remove_punct = TRUE) |>
    tokens_remove(stopwords("english")) |>
    dfm() |>
    dfm_group(groups = President) |>
    dfm_trim(min_termfreq = 7, verbose = FALSE) |>
    textplot_wordcloud(comparison = TRUE)
```

Compare between speeches

```{r}
corpus_subset(data, 
              docid_ %in% c("2009-Obama", "2013-Obama")) |>
    tokens(remove_punct = TRUE) |>
    tokens_remove(stopwords("english")) |>
    dfm() |>
    dfm_group(groups = docid_) |>
    dfm_trim(min_termfreq = 5, verbose = FALSE) |>
    textplot_wordcloud(comparison = TRUE)
```

### Relative frequency

```{r}
freq_grouped <- dfmat_inaug |> textstat_frequency(groups=docid_)

keyword <- "america"

word_freq_target <- subset(freq_grouped, freq_grouped$feature %in% keyword)
data.frame(word_freq_target$frequency,word_freq_target$group)

ggplot(word_freq_target, aes(x = frequency, y = group)) +
    geom_col() + 
    scale_x_continuous(limits = c(0, 20), breaks = c(seq(0, 20, 2))) +
    labs(x = "Frequency", y = NULL,
         title = paste('Frequency of', keyword))

```

### Relative frequency by group

Ratio of words is estimated by dfm_weight ("prop" = proportion of the feature counts of total feature counts). See more at <https://quanteda.io/reference/dfm_weight.html>

```{r}

freq_grouped <- dfmat_inaug |> dfm_weight(scheme = "prop") |> textstat_frequency(groups=docid_, n = 10)
freq_grouped

ggplot(data = freq_grouped, aes(x = nrow(freq_grouped):1, y = frequency)) +
     geom_point() +
     facet_wrap(~ group, scales = "free") +
     coord_flip() +
     scale_x_continuous(breaks = nrow(freq_grouped):1,
                        labels = freq_grouped$feature) +
     labs(x = NULL, y = "Relative frequency")
```

### What are the problems?

How do we find the distinct characteristics of each document (president), relative to others?

The importance of words can be estimated by term frequency-inverse document frequency (*tf-idf*).

```{r}
freq_tfidf <- dfmat_inaug |> dfm_tfidf() |> textstat_frequency(groups=docid_, n = 10,force = TRUE)

ggplot(data = freq_tfidf, aes(x = nrow(freq_tfidf):1, y = frequency)) +
     geom_point() +
     facet_wrap(~ group, scales = "free") +
     coord_flip() +
     scale_x_continuous(breaks = nrow(freq_tfidf):1,
                        labels = freq_tfidf$feature) +
     labs(x = NULL, y = "Relative frequency (tf-idf")
```

#END
